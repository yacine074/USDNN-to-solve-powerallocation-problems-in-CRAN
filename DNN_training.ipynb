{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IgV0qSY702fT"
   },
   "source": [
    "## Packages  \n",
    "\n",
    "First you need to install all packages which are compatible with the latset version of python. \n",
    "\n",
    "Installed packages need to be upgraded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "!pip3 install -r requirements --user"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once packages are installed, you can run the next cell to check whether they work.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 5676,
     "status": "ok",
     "timestamp": 1622022141814,
     "user": {
      "displayName": "lacom digital",
      "photoUrl": "",
      "userId": "01338797937786397342"
     },
     "user_tz": -120
    },
    "id": "0-znimLQ2zlc"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "from multiprocessing import Pool\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.ops import nn\n",
    "\n",
    "\n",
    "py_file_location = \"...\"\n",
    "os.path.abspath(os.path.join(os.path.dirname(py_file_location), os.path.pardir))\n",
    "\n",
    "from src.close_policy import *\n",
    "from src.utils import *\n",
    "from src.metrics import *\n",
    "from src.DNN_metrics import *\n",
    "from src.DNN_model import *\n",
    "from src.loss_function import *\n",
    "from src.data_generator import *\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choose GPU if it is available "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU'), PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU'), PhysicalDevice(name='/physical_device:GPU:1', device_type='GPU')]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.eager.context._EagerDeviceContext at 0x7f6bea80a240>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(tf.config.list_physical_devices())\n",
    "tf.device('GPU:1') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qzSvFblBUGab"
   },
   "source": [
    "## Channel gain creation\n",
    "\n",
    "First, you can choose the type of channel to perform the simulations. To do this, simply select the type by entering the number of the desired channel type "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$h_{ij}$ where $i \\in$ \\{P, S\\}, $j \\in$ \\{P, S\\}, \\ $i$;\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| $h_{ij}$      | $th_{ij}$  |\n",
    "| ------------- | ---------|\n",
    "| training set  | test set |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) Let us first create the channel coefficient for the training set  ($h_{ij}$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "        1. Channel gain with gaussian fading [1]\n",
      "        2. Channel gain with Anne model [2]\n",
      "        3. Channel gain with Uniform distribution[3] \n",
      "        4. Channel gain with Rician fading [4]\n",
      "        5. Channel gain with Nakagami fading [5]\n",
      "        6.Exit/Quit\n",
      "        \n",
      "Select channel gain\n",
      "1\n",
      "Channel gain created\n"
     ]
    }
   ],
   "source": [
    "h_PP, h_PR, h_RP, h_SS, h_SR, h_RS, h_SP, h_PS = channel_type() # train data-type"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) Next, let us do the same thing for the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "        1. Channel gain with gaussian fading [1]\n",
      "        2. Channel gain with Anne model [2]\n",
      "        3. Channel gain with Uniform distribution[3] \n",
      "        4. Channel gain with Rician fading [4]\n",
      "        5. Channel gain with Nakagami fading [5]\n",
      "        6.Exit/Quit\n",
      "        \n",
      "Select channel gain\n",
      "1\n",
      "Channel gain created\n"
     ]
    }
   ],
   "source": [
    "t_h_PP, t_h_PR, t_h_RP, t_h_SS, t_h_SR, t_h_RS, t_h_SP, t_h_PS = channel_type() # test data-type"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3) Once the channel coefficient are created, you can convert them to channel gain "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert channel coefficient to channel gain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "g_PP, g_PR, g_RP, g_SS, g_SR, g_RS, g_SP, g_PS =\\\n",
    "np.power(h_PP, 2), np.power(h_PR, 2), np.power(h_RP, 2), np.power(h_SS, 2)\\\n",
    ", np.power(h_SR, 2), np.power(h_RS, 2), np.power(h_SP, 2), np.power(h_PS, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_g_PP, t_g_PR, t_g_RP, t_g_SS, t_g_SR, t_g_RS, t_g_SP, t_g_PS =\\\n",
    "np.power(t_h_PP, 2), np.power(t_h_PR, 2), np.power(t_h_RP, 2), np.power(t_h_SS, 2)\\\n",
    ", np.power(t_h_SR, 2), np.power(t_h_RS, 2), np.power(t_h_SP, 2), np.power(t_h_PS, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data processing\n",
    "\n",
    "After data creation, it must be filtred to avoide the division by zero on the custom loss function. \n",
    "In order to do this, we keep only channel gain greater then a fixed threshold $s$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "g_RP, g_PP, g_SR, g_PR, g_SS, g_RS, g_SP, g_PS = data_filter(g_RP, g_PP, g_SR, g_PR, g_SS, g_RS, g_SP, g_PS)\n",
    "\n",
    "t_g_RP, t_g_PP, t_g_SR, t_g_PR, t_g_SS, t_g_RS, t_g_SP, t_g_PS = data_filter(t_g_RP, t_g_PP, t_g_SR, t_g_PR, t_g_SS, t_g_RS, t_g_SP, t_g_PS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset generation\n",
    "\n",
    "Let us now make things a bit more interesting and create the dataset then save it on a specific path."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:red\"> Imoprtant: </span>  \n",
    "#### 1) Data size must be fixed\n",
    "#### 2) Skip next cells and move to data splitting part, if the dataset is already created\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = int(1E6)\n",
    "\n",
    "test_size = int(2E5) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # choose your directory path  \n",
    "project_sub_path = \"dataset\"\n",
    "  \n",
    "# Parent Directory path\n",
    "parent_dir = \"\"\n",
    "  \n",
    "# Path\n",
    "try : \n",
    "\n",
    "    path = os.path.join(parent_dir, project_sub_path)\n",
    "    os.mkdir(path)\n",
    "    print(\"Directory '% s' created\" % project_sub_path)\n",
    "except FileExistsError : \n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " training set generation finished\n"
     ]
    }
   ],
   "source": [
    "np.savez(os.path.join(parent_dir,project_sub_path,'dataset_train.npz'),\n",
    "         g_PP=g_PP,\n",
    "         g_PS=g_PS,\n",
    "         g_PR=g_PR,\n",
    "         g_SP=g_SP,\n",
    "         g_SS=g_SS,\n",
    "         g_SR=g_SR,\n",
    "         g_RP=g_RP,\n",
    "         g_RS=g_RS\n",
    "         ) \n",
    "\n",
    "print(\"\\n training set generation finished\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bruteforce (exhaustive search)\n",
    "to evaluate the generalization capability of our proposed DNN during training; \n",
    "a test set containing $2 × 10^5$ channel realisations $g_l$, as well as the optimal resource allocation policy ($\\alpha^*, P^*_{R}, P^*_{S}$) obtained by brute force (or exhaustive search) to evaluate the optimality\n",
    "gap of our predicted solution.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The bruteforce method is applied only for the test set to use it as a benchmark with the DNN predicted results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " test set generation finished\n"
     ]
    }
   ],
   "source": [
    "bf_results = benchmark_generator(np.stack([t_g_RP[:test_size], t_g_PP[:test_size], t_g_SR[:test_size], t_g_PR[:test_size], t_g_SS[:test_size], t_g_RS[:test_size], t_g_SP[:test_size], t_g_PS[:test_size]], axis=1))\n",
    "\n",
    "np.savez(os.path.join(parent_dir,project_sub_path,'dataset_test.npz'),\n",
    "         g_PP=t_g_PP[:test_size],\n",
    "         g_PS=t_g_PS[:test_size],\n",
    "         g_PR=t_g_PR[:test_size],\n",
    "         g_SP=t_g_SP[:test_size],\n",
    "         g_SS=t_g_SS[:test_size],\n",
    "         g_SR=t_g_SR[:test_size],\n",
    "         g_RP=t_g_RP[:test_size],\n",
    "         g_RS=t_g_RS[:test_size],\n",
    "         R_S=bf_results[:,8], \n",
    "         alpha=bf_results[:,9], \n",
    "         P_R=bf_results[:,10], \n",
    "         P_S=bf_results[:,11]\n",
    "         ) \n",
    "\n",
    "print(\"\\n test set generation finished\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AL2JOrM0NfVF"
   },
   "source": [
    "## Data splitting\n",
    "Now, we can load the dataset if it's already created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 2259,
     "status": "ok",
     "timestamp": 1621968228124,
     "user": {
      "displayName": "lacom digital",
      "photoUrl": "",
      "userId": "01338797937786397342"
     },
     "user_tz": -120
    },
    "id": "WpmQqKYxNWwg"
   },
   "outputs": [],
   "source": [
    "train_size = int(1E6)\n",
    "\n",
    "test_size = int(2E5) \n",
    "\n",
    "# # choose your directory path  \n",
    "project_sub_path = \"dataset\"\n",
    "  \n",
    "# Parent Directory path\n",
    "parent_dir = \"\"\n",
    "\n",
    "\n",
    "### Train ###\n",
    "\n",
    "dataset_train = np.load(os.path.join(parent_dir,project_sub_path,'dataset_train.npz'))\n",
    "\n",
    "g_PP_tr = dataset_train['g_PP'][:train_size]\n",
    "g_PS_tr = dataset_train['g_PS'][:train_size]\n",
    "g_PR_tr = dataset_train['g_PR'][:train_size]\n",
    "g_SP_tr = dataset_train['g_SP'][:train_size]\n",
    "g_SS_tr = dataset_train['g_SS'][:train_size]\n",
    "g_SR_tr = dataset_train['g_SR'][:train_size]\n",
    "g_RP_tr = dataset_train['g_RP'][:train_size]\n",
    "g_RS_tr = dataset_train['g_RS'][:train_size]\n",
    "\n",
    "x_train = np.stack([g_RP_tr, g_PP_tr, g_SR_tr, g_PR_tr, g_SS_tr, g_RS_tr, g_SP_tr, g_PS_tr], axis=1)\n",
    "\n",
    "\n",
    "### Test ### \n",
    "\n",
    "dataset_test = np.load(os.path.join(parent_dir,project_sub_path,'dataset_test.npz'))\n",
    "\n",
    "\n",
    "g_PP_ts = dataset_test['g_PP'][:test_size]\n",
    "g_PS_ts = dataset_test['g_PS'][:test_size]\n",
    "g_PR_ts = dataset_test['g_PR'][:test_size]\n",
    "g_SP_ts = dataset_test['g_SP'][:test_size]\n",
    "g_SS_ts = dataset_test['g_SS'][:test_size]\n",
    "g_SR_ts = dataset_test['g_SR'][:test_size]\n",
    "g_RP_ts = dataset_test['g_RP'][:test_size]\n",
    "g_RS_ts = dataset_test['g_RS'][:test_size]\n",
    "\n",
    "R_S_ts = dataset_test['R_S']\n",
    "alpha_ts = dataset_test['alpha']#Alpha\n",
    "P_R_ts = dataset_test['P_R']#p_R\n",
    "P_S_ts = dataset_test['P_S']#p_S\n",
    "\n",
    "\n",
    "x_test = np.stack([g_RP_ts, g_PP_ts, g_SR_ts, g_PR_ts, g_SS_ts, g_RS_ts, g_SP_ts, g_PS_ts], axis=1)\n",
    "\n",
    "y_test = np.stack([R_S_ts, alpha_ts, P_R_ts, P_S_ts], axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DNN Training "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our numerical simulations, we used the\n",
    "ADAM optimizer [1] to iteratively update the weights of our\n",
    "DNN. The batch size is set to $4096$, the learning rate to $10^{−4}$;\n",
    "these values allows the DNN weights optimization to converge\n",
    "within $1000$ epochs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[[1]](https://arxiv.org/abs/1412.6980): Kingma, Diederik P., and Jimmy Ba. \"Adam: A method for stochastic optimization.\" arXiv preprint arXiv:1412.6980 (2014)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Impact of the hyperparameter $\\lambda$\n",
    "In the training stage, we need to investigate precisely the impact of the $\\lambda$ on the loss function. To do this, we choose an interval for this hyperparameter, $\\lambda \\in [10^{-1},...,10^{2}]$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## System setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "Lambda, tau, P_P = 10**(0.5), 0.25, 10.0\n",
    "\n",
    "metrics = [opportunistic_rate_DF(Lambda, tau, P_P), outage_DF(Lambda, tau, P_P), delta_DF(Lambda, tau, P_P), delta_out_DF(Lambda, tau, P_P)] \n",
    "\n",
    "Epochs = 1000 # Epochs number\n",
    "\n",
    "BS = 4096 # Batch_size\n",
    "\n",
    "VS = 0.2 # Validation set\n",
    "#'10_-1':10**-1, '10_-0.75':10**-0.75, \n",
    "LD = {'10_-0.5':10**-0.5, '10_-0.25':10**-0.25, '10_0':10**0, '10_0.25':10**0.25, '10_0.5':10**0.5, '10_0.75':10**0.75, '10_1':10**1, '10_1.25':10**1.25, '10_1.5':10**1.5, '10_1.75':10**1.75, '10_2':10**2}\n",
    "\n",
    "LR = 10**-4 #{'10_-4':10**-4} # One value or Dict to train the model with several learning rate\n",
    "\n",
    "root_dir ='DNN'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DNN training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1615370,
     "status": "ok",
     "timestamp": 1621954092361,
     "user": {
      "displayName": "lacom digital",
      "photoUrl": "",
      "userId": "01338797937786397342"
     },
     "user_tz": -120
    },
    "id": "yfQaip6vBGgA",
    "outputId": "c37a34a6-a401-4e89-b28d-a97354975db0"
   },
   "outputs": [],
   "source": [
    "for ld_k, ld_v in LD.items():\n",
    "    \n",
    "    #Create a new directory to save the network history and weights for each value of lambda \n",
    "\n",
    "    lambda_dir = root_dir+'/lambda = '+ld_k+'/weights/'\n",
    "  \n",
    "    history_dir = root_dir+'/lambda = '+ld_k+'/history/'\n",
    "\n",
    "    tf.io.gfile.makedirs(lambda_dir)\n",
    "\n",
    "    tf.io.gfile.makedirs(history_dir)\n",
    "\n",
    "    #for lr_k, lr_v in LR.items(): add a loop if browsing learning rate needed\n",
    "\n",
    "    model = get_model_DF(x_train, loss_DF(ld_v,tau), metrics,'sigmoid', custom_sigmoid, custom_sigmoid, LR) #lr_v\n",
    "    history = model.fit(x_train, x_train, epochs=Epochs, batch_size=BS, validation_split = VS)\n",
    "\n",
    "    model.save(lambda_dir+ld_k+'.h5')\n",
    "    np.save(history_dir+ld_k+'.npy',history.history)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "main.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
